# Overview

This project implements a kitchen brigade–style agentic workflow using LangGraph, with support for either OpenAI’s gpt-4o or IBM’s watsonx LLMs. It performs Retrieval-Augmented Generation (RAG) over a public recipe corpus, plans cooking subtasks assigned to specific brigade roles (e.g., sous-chef, saucier), executes those subtasks via dedicated LLM agents, and finally aggregates the results into a complete recipe. After cooking, two separate judge agents evaluate (1) the quality of the planning assignments and (2) the efficiency and effectiveness of the execution, providing structured feedback.

## Features:

- Configurable LLM backend: Switch between OpenAI and Watsonx models via command line option

- RAG with open-source recipes: Uses the formido/recipes dataset from Hugging Face

- Embeddings & FAISS search: Leverages all-MiniLM-L6-v2 sentence embeddings for retrieval, and indexes them with FAISS for fast similarity lookup

- Agentic “brigade” architecture: Models each kitchen role (chef de cuisine, commis, pâtissier, etc.) as an LLM agent node in a LangGraph workflow

- Four-stage workflow:
  - Retriever: Fetch relevant recipe snippets
  - Planner: Generate and assign subtasks to roles
  - Executor: Dispatch each subtask to the appropriate agent
  - Aggregator: Combine execution results into a final recipe
  - Automated judges: Two post-hoc evaluations—Planning Judge and Execution Judge—provide qualitative feedback on the workflow

# Installation

1. Clone the repository
```
    git clone https://github.com/DenizAskinIBM/Agentic-Kitchen-Brigade.git
    cd Agentic-Kitchen-Brigade
```

2. Create & activate a Python virtual environment
```
    python3 -m venv venv
    source venv/bin/activate
```

3.	Install dependencies
```
    pip install -r requirements.txt
```

4.	Install core libraries
```
    pip install langgraph                  # LangGraph orchestration  [oai_citation:5‡PyPI](https://pypi.org/project/langgraph/?utm_source=chatgpt.com)
    pip install python-dotenv              # Load .env credentials  [oai_citation:6‡PyPI](https://pypi.org/project/python-dotenv/?utm_source=chatgpt.com)
    pip install faiss-cpu                  # FAISS vector store  [oai_citation:7‡PyPI](https://pypi.org/project/faiss/?utm_source=chatgpt.com)
    pip install sentence-transformers      # Embeddings (all-MiniLM)  [oai_citation:8‡Hugging Face](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?utm_source=chatgpt.com)
    pip install datasets                   # Hugging Face Datasets  
    pip install openai ibm-watsonx         # LLM clients  
```

# Configuration

Create a .env file in the project root:
```
    OPENAI_API_KEY=your_openai_key_here
    WATSONX_URL=https://your-watsonx-instance
    WATSONX_APIKEY=your_watsonx_key_here
```

# Usage

```
usage: Kitchen_Brigade [-h] --dish DISH --crew CREW [--recipe RECIPE]
                       [--provider PROVIDER] [--model MODEL]
                       [--output-directory OUTPUT_DIRECTORY]
                       [--generated-recipe GENERATED_RECIPE]
                       [--final-recipe FINAL_RECIPE] [--execution-plan EXECUTION_PLAN]
                       [--plan-feedback PLAN_FEEDBACK]
                       [--execution-feedback EXECUTION_FEEDBACK]

options:
  -h, --help            show this help message and exit
  --dish DISH, -d DISH  The name of the dish to prepare. Used to create a recipe if
                        none is provided.
  --crew CREW, -c CREW  File defining the available roles and number of team members
                        in each role.
  --recipe RECIPE, -r RECIPE
                        Text file containing the recipe to prepare. Will be generated
                        via RAG if not supplied.
  --provider PROVIDER, -p PROVIDER
                        Name of the planning and judging model provider. One of:
                        openai (default), or watsonx
  --model MODEL, -m MODEL
                        Name of the model to use of planning and judging. Defaults to
                        'gpt-4o'

Output files:
  --output-directory OUTPUT_DIRECTORY, -o OUTPUT_DIRECTORY
                        Path to directory for output files.
  --generated-recipe GENERATED_RECIPE, --gr GENERATED_RECIPE
                        Output file for the recipe generated by RAG if no --recipe is
                        provided. Default: generated-recipe.txt
  --final-recipe FINAL_RECIPE, --fr FINAL_RECIPE
                        Output file for the final recipe after planning. Default:
                        final-recipe.txt
  --execution-plan EXECUTION_PLAN, --ep EXECUTION_PLAN
                        Output file for the generated preparation plan. Default:
                        execution-plan.txt
  --plan-feedback PLAN_FEEDBACK, --pf PLAN_FEEDBACK
                        File for the output of the Planning Judge. Default: plan-
                        feedback.txt
  --execution-feedback EXECUTION_FEEDBACK, --ef EXECUTION_FEEDBACK
                        File for the output of the Execution Judge. Default:
                        execution-feedback.txt
```

# Replicating Results

Looking to replicate our results from our [series of Medium articles](https://medium.com/@cwkirby/are-generative-models-good-planners-part-i-e20bf381f362)? Here are the command lines to do that. All assume you are invoking the script from the root of the repository and running with an appropriately configured virtual environment.

*Baseline*
```
python Kitchen_Brigade.py -d "Roast Chicken with Root Vegetables" -r results/baseline/generated-recipe.txt -c results/baseline/brigade.json -o results/baseline
```

*Grilled Cheese Sandwich w/Brigade*
```
python Kitchen_Brigade.py -d "Grilled Cheese Sandwich" -r results/grilled-cheese/grilled-cheese-recipe.txt -c results/grilled-cheese/brigade/brigade.json -o results/grilled-cheese/brigade
```

*Grilled Cheese Sandwich w/Small Crew*
```
python Kitchen_Brigade.py -d "Grilled Cheese Sandwich" -r results/grilled-cheese/grilled-cheese-recipe.txt -c results/grilled-cheese/small-crew/small-crew.json -o results/grilled-cheese/small-crew
```

*Grilled Cheese Sandwich w/Short Order Cook*
```
python Kitchen_Brigade.py -d "Grilled Cheese Sandwich" -r results/grilled-cheese/grilled-cheese-recipe.txt -c results/grilled-cheese/short-order/short-order.json -o results/grilled-cheese/short-order
```

*Grilled Cheese Sandwich w/Home Cook
```
python Kitchen_Brigade.py -d "Grilled Cheese Sandwich" -r results/grilled-cheese/grilled-cheese-recipe.txt -c results/grilled-cheese/home-cook/home-cook.json -o results/grilled-cheese/home-cook
```

# Code Structure

- kitchen_brigade.py: Main entry point
- LLMWrapper: Abstracts OpenAI/Watsonx calls
- RAG Setup: Loads recipes, builds embeddings & FAISS index
- Agent Definitions: Maps kitchen roles to LLM agents
- LangGraph Workflow: Defines nodes (retriever, planner, executor, aggregator) and edges
- Judges: Two additional LLM prompts that evaluate planning and execution quality

# Extending & Customizing

- Add new roles by updating the kitchen_roles list.
- Swap datasets by changing the HF load path in the RAG setup.
- Adjust retrieval size via retrieve(query, k=…).
- Refine prompts in the planner, executor, and judge nodes for domain-specific cooking styles.

# Contributing

1. Fork the repo
2. Create a branch (`git checkout -b feature/your-feature`)
3. Commit changes (`git commit -m 'Add some feature'`)
4. Push to branch (`git push origin feature/your-feature`)
5. Open a Pull Request

Please follow our Code of Conduct and Contributing Guide.

# License

This project is licensed under the MIT License. See LICENSE for details.